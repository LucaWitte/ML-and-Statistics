# -*- coding: utf-8 -*-
"""
Created on Fri Nov 18 08:14:38 2022

@author: Luca Witte
"""

from sklearn.datasets import load_iris
import numpy as np
import pandas as pd
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score
from sklearn.metrics import confusion_matrix
from sklearn.preprocessing import StandardScaler
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import KFold
import math


iris = load_iris()
x = iris.data # Contains measurements of 4 attributes
y = iris.target # Contains classification in 3 groups

feature_names = iris.feature_names
target_names = iris.target_names


#%% Part A & B

# Splits data in 2 subsets with observations and classifications each

def split_data(X,Y, att_ind, theta):
    X_set_true_temp = X[X[:,att_ind] >= theta,:]
    y_set_true_temp = y[X[:,att_ind] >= theta]
    
    X_set_false_temp = X[X[:,att_ind] < theta,:]
    y_set_false_temp = y[X[:,att_ind] < theta]
    
    return X_set_true_temp, y_set_true_temp, X_set_false_temp, y_set_false_temp
    
# Computes information content of whole dataset

def compute_information_content(y):
    prob_array = np.zeros([3,2])
    prob_array[:,0] = range(0,3)
    for i in range(0,3):
        counter = 0
        for j in range(np.shape(y)[0]):
            if y[j] == i: 
                counter += 1
        prob = counter/np.shape(y)[0]        
        try:
            prob_array[i,1] = -prob * math.log2(prob)
        except ValueError:
            prob_array[i,1] = 0 # CHECK THIS??
            
    return sum(prob_array[:,1])

# Computes information content of subset

def compute_information_a(X, Y, att_ind, theta):
    # split the dataset
    X_set_true, y_set_true, X_set_false, y_set_false = \
        split_data(x,y, att_ind, theta)

    Inf_1 = compute_information_content(y_set_false)
    Inf_2 = compute_information_content(y_set_true)
    size_y = Y.size
    
    return y_set_false.size/size_y * Inf_1 + \
                y_set_true.size/size_y * Inf_2
        
def compute_information_gain(X, Y, att_ind, theta):
    out = compute_information_content(Y) - \
            compute_information_a(X, Y, att_ind, theta)
    print("Split (", feature_names[att_ind], "<", theta,"): information gain = ", round(out,2))
            
compute_information_gain(x,y,0,5.5)  
compute_information_gain(x,y,1,3)  
compute_information_gain(x,y,2,2)  
compute_information_gain(x,y,3,1) 

#%% Part D

kf = KFold(n_splits=5, shuffle = True)
kf.get_n_splits(x)
kf.split(x)

accuracy_sum = 0
counter = 0

feature_imp = pd.DataFrame(np.zeros([5,4]))
feature_imp.columns = feature_names

for train_temp, test_temp in kf.split(x):
    
    x_train, x_test = x[train_temp], x[test_temp]
    y_train, y_test = y[train_temp], y[test_temp]
    
    classifier = DecisionTreeClassifier().fit(x_train,y_train)
    feature_imp.iloc[counter,:] = classifier.feature_importances_
    
    y_test_pred = classifier.predict(x_test)
    
    accuracy_sum += accuracy_score(y_test, y_test_pred)

    counter += 1

print(accuracy_sum/counter*100)
print(feature_imp)
print(feature_imp.sum(axis='rows'))
      
x_trimmed = x[y != 2]
y_trimmed = y[y != 2]

# Second subtask
      
kf = KFold(n_splits=5, shuffle = True)

accuracy_sum = 0
counter = 0

feature_imp = pd.DataFrame(np.zeros([5,4]))
feature_imp.columns = feature_names

for train_temp, test_temp in kf.split(x_trimmed):
    
    x_train, x_test = x_trimmed[train_temp], x_trimmed[test_temp]
    y_train, y_test = y_trimmed[train_temp], y_trimmed[test_temp]
    
    classifier = DecisionTreeClassifier().fit(x_train,y_train)
    feature_imp.iloc[counter,:] = classifier.feature_importances_
    
    y_test_pred = classifier.predict(x_test)
    
    accuracy_sum += accuracy_score(y_test, y_test_pred)

    counter += 1

print(accuracy_sum/counter*100)
print(feature_imp)
print(feature_imp.sum(axis='rows'))

